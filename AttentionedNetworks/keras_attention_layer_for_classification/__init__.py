"""
Attention on RNN that works for classification
Or Many-to-one attention mechanism

check for details:
https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566
https://gist.github.com/MLWhiz/b5ea274360600f4d7a7935d988edc2dc#file-keras_attention-py

25000/25000 [==============================] - 157s 6ms/sample - loss: 0.1849 - acc: 0.9316 - val_loss: 0.3161 - val_acc: 0.8700

"""